{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTiB E2024 - Week 12 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical exercises\n",
    "\n",
    "***Exercise 1***: How many terms are there in the sum on slide 13 from the lecture on Nov 18 for computing $P({\\bf X}|\\Theta)$? Why?\n",
    "\n",
    "- Z = k^n\n",
    "where k is the number of states\n",
    "\n",
    "***Exercise 2***: How many terms are there in the maximization on slide 68 in the Viterbi decoding slides from the lecure on Nov 18 for computing the Viterbi decoding ${\\bf Z}^*$? Why?\n",
    "- k^2 * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical exercises\n",
    "\n",
    "You are given the same 7-state HMM and helper functions that you used last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmm:\n",
    "    def __init__(self, init_probs, trans_probs, emission_probs):\n",
    "        self.init_probs = init_probs\n",
    "        self.trans_probs = trans_probs\n",
    "        self.emission_probs = emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_probs_7_state = [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00]\n",
    "\n",
    "trans_probs_7_state = [\n",
    "    [0.00, 0.00, 0.90, 0.10, 0.00, 0.00, 0.00],\n",
    "    [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.90, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
    "    [0.00, 0.00, 0.00, 0.10, 0.90, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "emission_probs_7_state = [\n",
    "    #   A     C     G     T\n",
    "    [0.30, 0.25, 0.25, 0.20],\n",
    "    [0.20, 0.35, 0.15, 0.30],\n",
    "    [0.40, 0.15, 0.20, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.20, 0.40, 0.30, 0.10],\n",
    "    [0.30, 0.20, 0.30, 0.20],\n",
    "    [0.15, 0.30, 0.20, 0.35],\n",
    "]\n",
    "\n",
    "hmm_7_state = hmm(init_probs_7_state, trans_probs_7_state, emission_probs_7_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_observations_to_indices(obs):\n",
    "    mapping = {'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
    "    return [mapping[symbol.lower()] for symbol in obs]\n",
    "\n",
    "def translate_indices_to_observations(indices):\n",
    "    mapping = ['a', 'c', 'g', 't']\n",
    "    return ''.join(mapping[idx] for idx in indices)\n",
    "\n",
    "def translate_path_to_indices(path):\n",
    "    return list(map(lambda x: int(x), path))\n",
    "\n",
    "def translate_indices_to_path(indices):\n",
    "    return ''.join([str(i) for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 - Viterbi Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will implement and experiment with the Viterbi algorithm. The implementation has been split into three parts:\n",
    "\n",
    "1. Fill out the $\\omega$ table using the recursion presented at the lecture.\n",
    "2. Find the state with the highest probability after observing the entire sequence of observations.\n",
    "3. Backtrack from the state found in the previous step to obtain the optimal path.\n",
    "\n",
    "We'll be working with the 7-state model (`hmm_7_state`) and the helper function for translating between observations, hidden states, and indicies, as introduced above (and also used last week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you're given the function below that constructs a table of a specific size filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_table(m, n):\n",
    "    \"\"\"Make a table with `m` rows and `n` columns filled with zeros.\"\"\"\n",
    "    return [[0] * n for _ in range(m)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be testing your code with the same two sequences as last week, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_long = 'TGAGTATCACTTAGGTCTATGTCTAGTCGTCTTTCGTAATGTTTGGTCTTGTCACCAGTTATCCTATGGCGCTCCGAGTCTGGTTCTCGAAATAAGCATCCCCGCCCAAGTCATGCACCCGTTTGTGTTCTTCGCCGACTTGAGCGACTTAATGAGGATGCCACTCGTCACCATCTTGAACATGCCACCAACGAGGTTGCCGCCGTCCATTATAACTACAACCTAGACAATTTTCGCTTTAGGTCCATTCACTAGGCCGAAATCCGCTGGAGTAAGCACAAAGCTCGTATAGGCAAAACCGACTCCATGAGTCTGCCTCCCGACCATTCCCATCAAAATACGCTATCAATACTAAAAAAATGACGGTTCAGCCTCACCCGGATGCTCGAGACAGCACACGGACATGATAGCGAACGTGACCAGTGTAGTGGCCCAGGGGAACCGCCGCGCCATTTTGTTCATGGCCCCGCTGCCGAATATTTCGATCCCAGCTAGAGTAATGACCTGTAGCTTAAACCCACTTTTGGCCCAAACTAGAGCAACAATCGGAATGGCTGAAGTGAATGCCGGCATGCCCTCAGCTCTAAGCGCCTCGATCGCAGTAATGACCGTCTTAACATTAGCTCTCAACGCTATGCAGTGGCTTTGGTGTCGCTTACTACCAGTTCCGAACGTCTCGGGGGTCTTGATGCAGCGCACCACGATGCCAAGCCACGCTGAATCGGGCAGCCAGCAGGATCGTTACAGTCGAGCCCACGGCAATGCGAGCCGTCACGTTGCCGAATATGCACTGCGGGACTACGGACGCAGGGCCGCCAACCATCTGGTTGACGATAGCCAAACACGGTCCAGAGGTGCCCCATCTCGGTTATTTGGATCGTAATTTTTGTGAAGAACACTGCAAACGCAAGTGGCTTTCCAGACTTTACGACTATGTGCCATCATTTAAGGCTACGACCCGGCTTTTAAGACCCCCACCACTAAATAGAGGTACATCTGA'\n",
    "z_long = '3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to translate these sequences to indices before using them with your algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing without log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement the algorithm without log-transformation. This will cause issues with numerical stability (like above when computing the joint probability), so we will use the log-transformation trick to fix this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_w(model, x):\n",
    "    k = len(model.init_probs)\n",
    "    n = len(x)\n",
    "    x = translate_observations_to_indices(x)\n",
    "    w = make_table(k, n)\n",
    "    #making the first column (the base cases)\n",
    "    for row in range(0,k): #going over all the k rows\n",
    "        w[row][0] = model.init_probs[row] * model.emission_probs[row][x[0]] #calculating the base case for each row (initial probability times emission probability)\n",
    "    #making the rest of the table (the iterative cases)\n",
    "    for column in range(1, n): #going over all the columns in teh table except teh first one which is already filled out\n",
    "        for row in range(0, k): #going over all the rows in each column\n",
    "            w[row][column] = model.emission_probs[row][x[column]] * max((w[h][column-1] * model.trans_probs[h][row]) for h in range(k)) #calculating the value to put in each cell (emission probability times the maximum of the previous state multiplied with the transition probability)   \n",
    "    return w #return the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function that given the $\\omega$-table, returns the probability of an optimal path through the HMM. As explained in the lecture, this corresponds to finding the highest probability in the last column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_path_prob(w):\n",
    "    return max(w[row][-1] for row in range(len(w))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your implementation in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9114255184318858e-31"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_short)\n",
    "opt_path_prob(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_long)\n",
    "opt_path_prob(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backtracking to find a most probable path of hidden states given the $\\omega$-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # REMEMBER TO USE math.isclose(a, b) when comparing floats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrack(model, x, w):\n",
    "    x = translate_observations_to_indices(x)  # Convert the observations to indices\n",
    "    path = [] #initialize list\n",
    "    prob = opt_path_prob_log(w)  # Get the probability\n",
    "    current_state = [i for i in range(len(w)) if w[i][-1] == prob][0] #accessing the state that matches the prob\n",
    "    path.insert(0, current_state)  # Add the last state to the path\n",
    "    k = len(w) #setting the total number of states\n",
    "    n = len(x) #setting the total number of time steps\n",
    "    current_time = n - 1  # Start from the last time step\n",
    "    # Backtrack through the table starting at the next to last column\n",
    "    for column in range(n - 2, -1, -1):\n",
    "        for row in range(k):\n",
    "            # Compute the candidate probability for this row\n",
    "            prob_candidate = model.emission_probs[current_state][x[current_time]] * w[row][column] * model.trans_probs[row][current_state]\n",
    "            if math.isclose(prob, prob_candidate):\n",
    "                previous_state = row #setting the state previous to the current state to be equal to this row\n",
    "                new_prob = w[row][column] #setting the new_prob to equal what is in this cell in the w table\n",
    "                if math.isclose(prob_candidate, 0):\n",
    "                    return \"Probability underflow occurred (sequence may be too long or model probabilities too small)\"\n",
    "        path.insert(0, previous_state)  # Insert the previous state into the path\n",
    "        current_state = previous_state  # Move to the previous state\n",
    "        current_time = column  # Move to the previous time step\n",
    "        prob = new_prob #setting prob to equal the value in the w table that matches the new time and state\n",
    "    return translate_indices_to_path(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33333333333321021021021021021021021021021021021021'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_short)\n",
    "z_viterbi = backtrack(hmm_7_state, x_short, w)\n",
    "z_viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probability underflow occurred (sequence may be too long or model probabilities too small)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_long)\n",
    "z_viterbi = backtrack(hmm_7_state, x_long, w)\n",
    "z_viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing with log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the Viterbi algorithm with log-transformation. The steps are the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the (log-transformed) $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_w_log(model, x):\n",
    "    x = translate_observations_to_indices(x)\n",
    "    k = len(model.init_probs)\n",
    "    n = len(x)\n",
    "    w = make_table(k, n)\n",
    "    for row in range(0,k):\n",
    "        w[row][0] = log(model.init_probs[row]) + log(model.emission_probs[row][x[0]])\n",
    "    for column in range(1, n):\n",
    "        for row in range(0, k):\n",
    "            w[row][column] = log(model.emission_probs[row][x[column]]) + (max((w[h][column-1] + log(model.trans_probs[h][row])) for h in range(k)))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the (log-transformed) joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_path_prob_log(w):\n",
    "    k = len(w)\n",
    "    max_val = float('-inf')\n",
    "    for i in range(k):\n",
    "        if w[i][-1] > max_val:\n",
    "            max_val = w[i][-1]\n",
    "    return max_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-70.73228857440488"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_short)\n",
    "opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1406.7209253880144"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_long)\n",
    "opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def backtrack_log(model, x, w):\n",
    "    x = translate_observations_to_indices(x)  # Convert the observations to indices\n",
    "    path = [] #initialize list\n",
    "    prob = opt_path_prob_log(w)  # Get the probability\n",
    "    current_state = [i for i in range(len(w)) if w[i][-1] == prob][0] #accessing the state that matches the prob\n",
    "    path.insert(0, current_state)  # Add the last state to the path\n",
    "    k = len(w) #setting the total number of states\n",
    "    n = len(x) #setting the total number of time steps\n",
    "    current_time = n - 1  # Start from the last time step\n",
    "    # Backtrack through the table starting at the next to last column\n",
    "    for column in range(n - 2, -1, -1):\n",
    "        for row in range(k):\n",
    "            # Compute the candidate probability for this row\n",
    "            prob_candidate = log(model.emission_probs[current_state][x[current_time]]) + w[row][column] + log(model.trans_probs[row][current_state])\n",
    "            if math.isclose(prob, prob_candidate):\n",
    "                previous_state = row #setting the state previous to the current state to be equal to this row\n",
    "                new_prob = w[row][column] #setting the new_prob to equal what is in this cell in the w table\n",
    "                if math.isclose(prob_candidate, -math.inf):\n",
    "                    return \"Probability underflow occurred (sequence may be too long or model probabilities too small)\"\n",
    "        path.insert(0, previous_state)  # Insert the previous state into the path\n",
    "        current_state = previous_state  # Move to the previous state\n",
    "        current_time = column  # Move to the previous time step\n",
    "        prob = new_prob #setting prob to equal the value in the w table that matches the new time and state\n",
    "    return translate_indices_to_path(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33333333333321021021021021021021021021021021021021'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_short)\n",
    "z_viterbi_log = backtrack_log(hmm_7_state, x_short, w)\n",
    "z_viterbi_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_long)\n",
    "z_viterbi_log = backtrack_log(hmm_7_state, x_long, w)\n",
    "z_viterbi_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how to verify that your implementations of Viterbi (i.e. `compute_w`, `opt_path_prob`, `backtrack`, and there log-transformed variants `compute_w_log`, `opt_path_prob_log`, `backtrack_log`) are correct.\n",
    "\n",
    "One thing that should hold is that the probability of a most likely path as computed by `opt_path_prob` (or `opt_path_prob_log`) for a given sequence of observables (e.g. `x_short` or `x_long`) should be equal to the joint probability of a corersponding most probable path as found by `backtrack` (or `backtrack_log`) and the given sequence of observables. Why?\n",
    "\n",
    "Make an experiment that validates that this is the case for your implementations of Viterbi and `x_short` and `x_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# To access joint_prob and joint_prob_log, you must copy your implementations from last week here ...\n",
    "\n",
    "def joint_prob(model, x, z):\n",
    "    prob = model.init_probs[z[0]]\n",
    "    for i in range(1, len(z)):\n",
    "        prob *= model.trans_probs[z[i-1]][z[i]]\n",
    "    for i in range(len(z)):\n",
    "        prob *= model.emission_probs[z[i]][x[i]]  \n",
    "    return prob\n",
    "\n",
    "def joint_prob_log(model, x, z):\n",
    "    prob = log(model.init_probs[z[0]])\n",
    "    for i in range(1, len(z)):\n",
    "        prob += log(model.trans_probs[z[i-1]][z[i]])\n",
    "    for i in range(len(z)):\n",
    "        prob += log(model.emission_probs[z[i]][x[i]])  \n",
    "    return prob\n",
    "\n",
    "\n",
    "# Check that opt_path_prob is equal to joint_prob(hmm_7_state, x_short, z_viterbi)\n",
    "print(math.isclose(opt_path_prob(compute_w(hmm_7_state, x_short)), joint_prob(hmm_7_state, translate_observations_to_indices(x_short), translate_path_to_indices(z_short))))\n",
    "\n",
    "# Check that opt_path_prob_log is equal to joint_prob_log(hmm_7_state, x_short, z_viterbi_log)\n",
    "print(math.isclose(opt_path_prob_log(compute_w_log(hmm_7_state, x_short)), joint_prob_log(hmm_7_state, translate_observations_to_indices(x_short), translate_path_to_indices(z_short))))\n",
    "\n",
    "# Do the above checks for x_long ...\n",
    "print(math.isclose(opt_path_prob(compute_w(hmm_7_state, x_long)), joint_prob(hmm_7_state, translate_observations_to_indices(x_long), translate_path_to_indices(z_long))))\n",
    "print(math.isclose(opt_path_prob_log(compute_w_log(hmm_7_state, x_long)), joint_prob_log(hmm_7_state, translate_observations_to_indices(x_long), translate_path_to_indices(z_long))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your implementations pass the above checks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, but only because the third print statement ends up comparing 0 and 0. So even though it passes, it compares two wrong values (which both happen to be 0, since the probabilities have become so small)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does log-transformation matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an experiment that investigates how long the input string can be before `backtrack` and `backtrack_log` start to disagree on a most likely path and its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(x_long), 10):\n",
    "    x = x_long[:i]\n",
    "    z = z_long[:i]\n",
    "    \n",
    "    x_trans = translate_observations_to_indices(x)\n",
    "    z_trans = translate_path_to_indices(z)\n",
    "    w = compute_w(hmm_7_state, x)\n",
    "    w_log = compute_w_log(hmm_7_state, x)\n",
    "\n",
    "    no_log = backtrack(hmm_7_state, x, w)\n",
    "    with_log = backtrack_log(hmm_7_state, x, w_log)\n",
    "    if not no_log == with_log:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "\n",
    "For the 7-state model, `backtrack` and `backtrack_log` start to disagree on a most likely path and its probability for **i = 524** if you run it with increments of 1. If you run it with increments of 10 (which is way faster and what we have done above) you get an approximate value of **i = 531**. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "CTiB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
